{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59c1e73a",
   "metadata": {},
   "source": [
    "<h1>Import libraries</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0795d91f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'snowpark'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#importing the required libraries\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msnowpark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msnowpark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msnowpark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m VectorAssembler\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'snowpark'"
     ]
    }
   ],
   "source": [
    "\n",
    "     #importing the required libraries\n",
    "from snowpark.sql.types import *\n",
    "from snowpark.sql.functions import *\n",
    "from snowpark.ml.feature import VectorAssembler\n",
    "from snowpark.ml.classification import LogisticRegression\n",
    "     \n",
    "     #creating a dataframe\n",
    "     df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"data.csv\")\n",
    "     \n",
    "     #creating a vector assembler\n",
    "     assembler = VectorAssembler() \\\n",
    "         .setInputCols([\"col1\", \"col2\", \"col3\"]) \\\n",
    "         .setOutputCol(\"features\")\n",
    "     \n",
    "     #transforming the dataframe\n",
    "     df = assembler.transform(df)\n",
    "     \n",
    "     #creating a logistic regression model\n",
    "     lr = LogisticRegression() \\\n",
    "         .setLabelCol(\"label\") \\\n",
    "         .setFeaturesCol(\"features\") \\\n",
    "         .setMaxIter(10) \\\n",
    "         .setRegParam(0.3) \\\n",
    "         .setElasticNetParam(0.8)\n",
    "     \n",
    "     #fitting the model\n",
    "     lrModel = lr.fit(df)\n",
    "     \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc8c9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Internal Library to register model\n",
    "!pip install fosforml\n",
    "from fosforml import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d1af6ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/31/12/461c29d25f91c246842fb09e9cfb21ab82a9ab8ea8dd35667cc8b708f715/pyspark-3.5.2.tar.gz (317.3MB)\n",
      "\u001b[K     |████████████████████████████▎   | 279.9MB 119.6MB/s eta 0:00:01"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 317.3MB 55kB/s \n",
      "\u001b[?25hCollecting py4j==0.10.9.7\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/10/30/a58b32568f1623aaad7db22aa9eafc4c6c194b429ff35bdc55ca2726da47/py4j-0.10.9.7-py2.py3-none-any.whl (200kB)\n",
      "\u001b[K     |████████████████████████████████| 204kB 99.4MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.5.2-py2.py3-none-any.whl size=317812371 sha256=76ceb6ceb8c4558a4c2ba52c0e6787793f52b2561a9a70c6e84ce01b90d7f190\n",
      "  Stored in directory: /home/mosaic-ai/.cache/pip/wheels/16/05/1d/844e7f1ed3f22d7706e14e9284e269cfc08e3b51b117edb4ac\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.7 pyspark-3.5.2\n",
      "\u001b[33mWARNING: You are using pip version 19.3.1; however, version 24.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d948516",
   "metadata": {},
   "source": [
    "<h1>Establish connection to snowflake</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a5f3b4b",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2244922424.py, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[2], line 7\u001b[0;36m\u001b[0m\n\u001b[0;31m    --from snowflake.snowpark.session import Session\u001b[0m\n\u001b[0m      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#estblishing connection between notebook and snowflake\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import time\n",
    "from snowflake.snowpark.session import Session\n",
    "import configparser\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read(\"snowflake_connection.ini\")\n",
    "\n",
    "connection_parameters = {\n",
    "    \"user\": f'{config[\"Snowflake\"][\"user\"]}',\n",
    "    \"password\": f'{config[\"Snowflake\"][\"password\"]}',\n",
    "    \"account\": f'{config[\"Snowflake\"][\"account\"]}',\n",
    "    \"WAREHOUSE\": f'{config[\"Snowflake\"][\"WAREHOUSE\"]}',\n",
    "    \"DATABASE\": f'{config[\"Snowflake\"][\"DATABASE\"]}',\n",
    "    \"SCHEMA\": f'{config[\"Snowflake\"][\"SCHEMA\"]}'\n",
    "}\n",
    "\n",
    "def snowflake_connector(conn):\n",
    "    try:\n",
    "        session = Session.builder.configs(conn).create()\n",
    "        print(\"connection successful!\")\n",
    "    except:\n",
    "        raise ValueError(\"error while connecting with db\")\n",
    "    return session\n",
    "\n",
    "session = snowflake_connector(connection_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e024a571-eaf9-4f38-8371-4d6802762a2c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "error while connecting with db",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 26\u001b[0m, in \u001b[0;36msnowflake_connector\u001b[0;34m(conn)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 26\u001b[0m     session \u001b[38;5;241m=\u001b[39m \u001b[43mSession\u001b[49m\u001b[38;5;241m.\u001b[39mbuilder\u001b[38;5;241m.\u001b[39mconfigs(conn)\u001b[38;5;241m.\u001b[39mcreate()\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconnection successful!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Session' is not defined",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 32\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror while connecting with db\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m session\n\u001b[0;32m---> 32\u001b[0m session \u001b[38;5;241m=\u001b[39m \u001b[43msnowflake_connector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconnection_parameters\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 29\u001b[0m, in \u001b[0;36msnowflake_connector\u001b[0;34m(conn)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconnection successful!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m---> 29\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror while connecting with db\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m session\n",
      "\u001b[0;31mValueError\u001b[0m: error while connecting with db"
     ]
    }
   ],
   "source": [
    "#estblishing connection between notebook and snowflake\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import time\n",
    "import configparser\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read(\"snowflake_connection.ini\")\n",
    "\n",
    "connection_parameters = {\n",
    "    \"user\": f'{config[\"Snowflake\"][\"user\"]}',\n",
    "    \"password\": f'{config[\"Snowflake\"][\"password\"]}',\n",
    "    \"account\": f'{config[\"Snowflake\"][\"account\"]}',\n",
    "    \"WAREHOUSE\": f'{config[\"Snowflake\"][\"WAREHOUSE\"]}',\n",
    "    \"DATABASE\": f'{config[\"Snowflake\"][\"DATABASE\"]}',\n",
    "    \"SCHEMA\": f'{config[\"Snowflake\"][\"SCHEMA\"]}'\n",
    "}\n",
    "\n",
    "def snowflake_connector(conn):\n",
    "    try:\n",
    "        session = Session.builder.configs(conn).create()\n",
    "        print(\"connection successful!\")\n",
    "    except:\n",
    "        raise ValueError(\"error while connecting with db\")\n",
    "    return session\n",
    "\n",
    "session = snowflake_connector(connection_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0742cc",
   "metadata": {},
   "source": [
    "<h1>Analyze data</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4f3c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_bangalore_2022 = session.table(\"SOL_ASSORTMENT_PLANNING_COMB\").to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a595dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_bangalore_2022.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f636159",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_final = session.table(\"SOL_ASSORTMENT_PLANNING_TRAIN_DATA_MODEL\").to_pandas()\n",
    "train_final.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89005e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_final.columns = train_final.columns.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f3f7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_final.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a21b7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "master_data = sales_bangalore_2022.copy()\n",
    "master_data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed19aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "master_data.columns = master_data.columns.str.lower()\n",
    "master_data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2b440d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=master_data.copy\n",
    "master_data['unique_id']=master_data['product_code']+'-'+ master_data['outlet_code']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55253a1",
   "metadata": {},
   "source": [
    "<h1>Train Model</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5023845a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(train_final[['frequencym','sales_value_avg','scheme_amount_perproduct']],train_final['sales_indicator'],test_size=0.3,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b1c4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('logreg.pkl', 'rb') as model_file:\n",
    "   loaded_model = pickle.load(model_file)\n",
    "probs = loaded_model.predict_proba(X_test)\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577066f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = 0.3\n",
    "y_pred = (probs[:,1]>=cutoff).astype(int)\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fda12a7",
   "metadata": {},
   "source": [
    "<h1> Test model </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578d8832",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_final = session.table(\"SOL_ASSORTMENT_PLANNING_TEST_SET_MODEL\").to_pandas()\n",
    "test_final.columns = test_final.columns.str.lower()\n",
    "test_final.head(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1d8340",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_unseen = test_final[['frequencym','sales_value_avg','scheme_amount_perproduct']]\n",
    "y_test_unseen = test_final['sales_indicator']\n",
    "print(X_test_unseen.shape,y_test_unseen.shape)\n",
    "probs = loaded_model.predict_proba(X_test_unseen)\n",
    "test_final['probablity']=probs[:,1]\n",
    "y_pred = (probs[:,1]>=0.4).astype(int)\n",
    "test_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d468719",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_final['prediction']= test_final['probablity'].apply(lambda x: 1  if x>=0.3 else 0)\n",
    "test_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7061f3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_final_static = test_final.copy()\n",
    "print(classification_report(test_final['sales_indicator'],test_final['prediction']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782ef425",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_final.reset_index(drop=True)\n",
    "test_final['OOS'] = test_final['probablity'].apply(lambda x:1 if x>0.3 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aee31e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "soq_master = master_data.groupby('unique_id').agg({'sales_units':'sum','mnth_code':'nunique'})\n",
    "soq_master['SOQ'] = soq_master['sales_units']/soq_master['mnth_code']\n",
    "soq_master.reset_index()\n",
    "final_reco = pd.merge(soq_master,test_final,on='unique_id',how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b776056f",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_reco = final_reco[['unique_id','prediction','SOQ','OOS']]\n",
    "final_reco.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5016b9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ms_reco = final_reco.copy()\n",
    "ms_reco.columns = ['unique_id', 'ms_flag', 'soq', 'oos_flag']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577153c4",
   "metadata": {},
   "source": [
    "<h1> Write output to a snowflake table </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f99e2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5ef296",
   "metadata": {},
   "outputs": [],
   "source": [
    "ms_reco[['product', 'outlet']] = ms_reco['unique_id'].str.split('-', expand=True)\n",
    "ms_reco.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe9d21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ms_reco=session.createDataFrame(\n",
    "        ms_reco.values.tolist(),\n",
    "        schema=ms_reco.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd06053",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ms_reco.write.mode(\"overwrite\").save_as_table(\"SOL_CPG_DB.SOL_SALES_SCHEMA.SOL_ASSORTMENT_PREDICTION_TABLE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba46d985",
   "metadata": {},
   "outputs": [],
   "source": [
    "req = requests.Request()\n",
    "req.json = {\"payload\":X_test_unseen.head(1).to_json()}\n",
    "print({'payload': req.json})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d636ba5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from fosforml import *\n",
    "\n",
    "@scoring_func\n",
    "def score(model, request):\n",
    "    payload = request.json[\"payload\"]\n",
    "    data = pd.DataFrame(eval(payload))\n",
    "    prediction = pd.DataFrame(model.predict(data))\n",
    "    return prediction.to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c468b8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(score(loaded_model, req))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8922a0",
   "metadata": {},
   "source": [
    "<h1>Register model in FDC</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e4814b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fosforml.constants import MLModelFlavours\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373ee012",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = register_model(loaded_model, \n",
    "               score, \n",
    "               name=\"Assortment_Planning_Prediction\", \n",
    "               description=\"prediction of assortment across retailers by product\",\n",
    "               flavour=MLModelFlavours.sklearn,\n",
    "               model_type=\"regression\",\n",
    "               y_true=y_test,\n",
    "               y_pred=y_pred, \n",
    "               prob=probs, \n",
    "               features=X_train.columns,\n",
    "               labels=[0,1],\n",
    "               input_type=\"json\", \n",
    "               explain_ai=True, \n",
    "               x_train=X_train, \n",
    "               x_test=X_test, \n",
    "               y_train=y_train.tolist(),\n",
    "               y_test=y_test.tolist(),\n",
    "               feature_names=X_train.columns.tolist(),\n",
    "               original_features=X_train.columns.tolist(),\n",
    "               feature_ids=X_train.columns,\n",
    "               target_names=['No Failure',' or Failure'],\n",
    "               kyd=True, kyd_score = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
