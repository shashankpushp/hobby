{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7daac170",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!sudo pip install -q transformers --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6180373b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124406a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c3d654",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    '/data/mistral/query-to-mql/exp-8/oct-30/merged-model',\n",
    "#     quantization_config=bnb_config,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\", trust_remote_code=True,\n",
    "                                          # add_eos_token=True,\n",
    "                                          use_fast=False)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# pipeline\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, device_map='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d71760",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_data = \"\"\"{\n",
    "    \"MEASURE\": [{\"ENTITY\": \"Discount\", \"other names\": [\"discount\", \"discount rate\", \"discount value\", \"deduction\"]},\n",
    "                {\"ENTITY\": \"Purchase Vol\", \"other names\": [\"purchase\", \"purchase value\", \"purchase model\"]},\n",
    "                {\"ENTITY\": \"Quantity\", \"other names\": [\"quantity\", \"volume\"]},\n",
    "                {\"ENTITY\": \"Sales\", \"other names\": [\"sales\", \"sale\"]}],\n",
    "    \"DIMENSION\": [{\"ENTITY\": \"Sub-Category\", \"other names\": [\"sub-category\", \"sub category\", \"categories\", \"section\"]},\n",
    "                  {\"ENTITY\": \"Segment\", \"other names\": [\"segment\", \"segments\", \"units\", \"divisions\"]},\n",
    "                  {\"ENTITY\": \"Parts\", \"other names\": [\"parts\", \"part\", \"section\", \"divisions\"]},\n",
    "                  {\"ENTITY\": \"Country\", \"other names\": [\"country\", \"countries\"]}],\n",
    "    \"FILTER\": [{\"ENTITY\": \"Consumer\", \"other names\": [\"consumers\", \"consumer\"], \"parent\": \"Segment\"},\n",
    "               {\"ENTITY\": \"Phone\", \"other names\": [\"phone\", \"phones\", \"mobile phones\"], \"parent\": \"Sub-Category\"},\n",
    "               {\"ENTITY\": \"Binder\", \"other names\": [\"binders\", \"binder\"], \"parent\": \"Sub-Category\"},\n",
    "               {\"ENTITY\": \"Corporate\", \"other names\": [\"corporates\", \"corporate\"], \"parent\": \"Segment\"},\n",
    "               {\"ENTITY\": \"India\", \"other names\": [\"india\"], \"parent\": \"Country\"},\n",
    "               {\"ENTITY\": \"Dubai\", \"other names\": [\"dubai\"], \"parent\": \"Country\"}],\n",
    "    \"DERIVED MEASURE\": [{\"ENTITY\": \"Ratio\",\n",
    "             \"other names\": [\"ratio\", \"share\", \"contribution\", \"percentage\", \"proportion\", \"contributing\"]},\n",
    "            {\"ENTITY\": \"Why\", \"other names\": [\"why\", \"cause of\", \"reason for\", \"diagnose\"]},\n",
    "            {\"ENTITY\": \"contribution_to_growth\", \"other names\": [\"contribution to growth\", \"growth\", \"grown\"]},\n",
    "            {\"ENTITY\": \"kda_transactional\", \"other names\": [\"kda\", \"key drivers\", \"key driver\", \"drivers\", \"driver\"]},\n",
    "            {\"ENTITY\": \"Growth Rate\", \"other names\": [\"growth rate\", \"growth\", \"grown\"]},\n",
    "            {\"ENTITY\": \"correlation\",\n",
    "             \"other names\": [\"associate\", \"associated\", \"association\", \"associations\", \"correlate\", \"correlated\",\n",
    "                             \"correlation\", \"correlations\", \"relate\", \"related\", \"relation\", \"relations\",\n",
    "                             \"relationship\",\n",
    "                             \"relationships\"]}\n",
    "            ],\n",
    "    \"DATE VARIABLE\": [{\"ENTITY\": \"Order Date\", \"other names\": [\"order date\", \"date\", \"trend\", \"time\", \"when\", \"mom\", \"yoy\"]}]\n",
    "    }\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c939e150",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_input_data = {\n",
    "    \"start_date\": \"01/01/2020\",\n",
    "    \"end_date\": \"15/09/2023\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb83dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_template = \"\"\"Given the context : {context} and date reference: {date_input}, the query: {user_query}, is converted into below shown structured output.\n",
    "[MQL]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67231b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "query = 'what is sales in 2020'\n",
    "prompt = query_template.format(context=context_data,\n",
    "                              date_input=date_input_data,\n",
    "                              user_query=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc2c42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1= time.time()\n",
    "pipe(prompt)\n",
    "t2= time.time()\n",
    "t2-t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842990d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name=\"/data/mistral/query-to-mql/exp-8/oct-30/merged-model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab8b0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True,\n",
    "                                          # add_eos_token=True,\n",
    "                                          use_fast=False)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfff9c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_template_query_v1(user_query):\n",
    "    prompt = query_template.format(context=context_data,\n",
    "                                   user_query=user_query,\n",
    "                                  date_input=date_input_data)\n",
    "    _inputs = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "#   outputs = model.generate(input_ids=_inputs.to('cuda'), max_length= 1700, pad_token_id=tokenizer.eos_token_id)\n",
    "    outputs = pipe(prompt, max_new_tokens=256)\n",
    "    print(outputs)\n",
    "#     output = tokenizer.decode(outputs[0])\n",
    "#     output_new = output.split('[MQL]\\n')[1]\n",
    "#     print(output_new.split('\\n[/MQL]')[0])\n",
    "#     return output_new.split('\\n[/MQL]')[0], output\n",
    "    \n",
    "#     return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18dc277b",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query = 'why sales changed in last 2 weeks of aug 2021'\n",
    "print('user query: ', user_query)\n",
    "print('-'*100)\n",
    "predict_template_query_v1(user_query=user_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1995e87",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "user_query = 'why sales changed in last 2 weeks of aug 2021'\n",
    "print('user query: ', user_query)\n",
    "print('-'*100)\n",
    "output, raw = predict_template_query_v1(user_query=user_query)\n",
    "print(eval(output))\n",
    "print('-'*100)\n",
    "print('Step 1:' +raw.split('\\nStep 1:')[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ea2eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fosforml import *\n",
    "from fosforml.constants import MLModelFlavours\n",
    "\n",
    "# # new score functions\n",
    "from mosaic_utils.ai.score.base import ScoreBase\n",
    "from typing import Tuple, Union, List, Any\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46554997",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScoreTemplateExample(ScoreBase):\n",
    "    \"\"\"\n",
    "    This Class Demonstrate How To Implements ScoreBase Interface Class And It Basic Usage.\n",
    "    \"\"\"    \n",
    "    def __init__(self, context_data, date_input_data):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model_loaded = None\n",
    "        \n",
    "        self.context_data = context_data\n",
    "        self.date_input_data = date_input_data\n",
    "\n",
    "        import torch\n",
    "        from transformers import pipeline\n",
    "\n",
    "        if self.model_loaded is None:\n",
    "            print(\"LLM model loading from data section\")\n",
    "            self.model_loaded = self.prepare_model()\n",
    "    \n",
    "    \n",
    "    def prepare_model(self):\n",
    "        # load model\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            '/data/mistral/query-to-mql/exp-8/oct-30/merged-model',\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "        model.config.use_cache = False\n",
    "        model.config.pretraining_tp = 1\n",
    "\n",
    "        # Load tokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\", trust_remote_code=True,\n",
    "                                                  use_fast=False)\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        tokenizer.padding_side = \"right\"\n",
    "        \n",
    "        #pipeline\n",
    "        pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, device_map='auto')\n",
    "        return pipe\n",
    "        \n",
    "    \n",
    "    def request_processing_fn(self, request) :\n",
    "        \"\"\"\n",
    "        Processes Request Object -> List[Input data, ..]. It could be:\n",
    "               A List Mapping of All Value Can Be one of : \n",
    "                   - List[ [Feature_Value1, Feature_Value2, ...], [...] ]\n",
    "                   - List[numpy.array(), numpy.array(), ...]\n",
    "                   - List[tf.Tensor, tf.Tensor, tf.Tensor, ...]\n",
    "                   - List[ SingleSample, SingleSample]\n",
    "\n",
    "        :return: (n_inputs, payload's)\n",
    "\n",
    "        Warnings:\n",
    "        1. Do not reshape your final output for single sample here, do it in prediction.\n",
    "           Else payloads will be invalidated for extraction at raw and extraction level.\n",
    "        \"\"\"\n",
    "        final_payload = []\n",
    "        raw_payload = request.json[\"payload\"]\n",
    "        return (1, raw_payload) \n",
    "    \n",
    "    def pre_processing_fn(self,payload):\n",
    "        return payload\n",
    "\n",
    "    def prediction_fn(self,\n",
    "                       model: Any,\n",
    "                       input_query \n",
    "                       ):\n",
    "        \"\"\"\n",
    "                Does the main prediction on pre_processed_input(Single Sample) using supplied model .\n",
    "\n",
    "                :param model: Supported Model\n",
    "                :param pre_processed_input: Single Preprocessed Payload\n",
    "                :return: Prediction Value From the model\n",
    "                \n",
    "                Important Notes:\n",
    "                - Reshape your data array.reshape(1, -1) before predictions as it contains a single sample.\n",
    "                    \n",
    "        \"\"\"\n",
    "        prompt_template = \"\"\"Given the context : {context} and date reference: {date_input}, the query: {user_query}, is converted into below shown structured output.\n",
    "[MQL]\n",
    "\"\"\"\n",
    "        text = input_query\n",
    "        prompt = prompt_template.format(context=self.context_data,\n",
    "                              date_input=self.date_input_data,\n",
    "                              user_query=text)\n",
    "        print(\"Input being passed to Model ...\")\n",
    "        outputs = self.model_loaded(prompt, max_new_tokens=1800)\n",
    "        preds = outputs[0][\"generated_text\"]\n",
    "        output_new = preds.split('[MQL]\\n')[1]\n",
    "        pred = output_new.split('\\n[/MQL]')[0]\n",
    "        print(\"prediction is \\n: \", pred)\n",
    "        return pred\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3f1bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_ = ScoreTemplateExample(context_data, date_input_data)\n",
    "import requests\n",
    "req = requests.Request()\n",
    "\n",
    "req.json = {\"payload\":\"How do I sort a list in Python?\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64d0742",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89060f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5655874",
   "metadata": {},
   "outputs": [],
   "source": [
    "t3 = time.time()\n",
    "model_predictions = score_.score(None, req, dry_run=True)\n",
    "t4 = time.time()\n",
    "print(t4-t3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
